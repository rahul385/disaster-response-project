{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "\n",
    "### 1. Import libraries and load data from database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RahulGupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RahulGupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords'])\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,classification_report,recall_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///data/DisasterResponse.db')\n",
    "df = pd.read_sql('disaster_response',engine)\n",
    "X = df.message\n",
    "Y = df.drop(['id','message','original','genre'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Weather update - a cold front from Cuba that c...\n",
      "1              Is the Hurricane over or is it not over\n",
      "2                      Looking for someone but no name\n",
      "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
      "4    says: west side of Haiti, rest of the country ...\n",
      "Name: message, dtype: object\n",
      "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
      "0        1        0      0            0             0                 0   \n",
      "1        1        0      0            1             0                 0   \n",
      "2        1        0      0            0             0                 0   \n",
      "3        1        1      0            1             0                 1   \n",
      "4        1        0      0            0             0                 0   \n",
      "\n",
      "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
      "0                  0         0         0            0  ...            0   \n",
      "1                  0         0         0            0  ...            0   \n",
      "2                  0         0         0            0  ...            0   \n",
      "3                  0         0         0            0  ...            0   \n",
      "4                  0         0         0            0  ...            0   \n",
      "\n",
      "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
      "0                     0                0       0      0     0           0   \n",
      "1                     0                1       0      1     0           0   \n",
      "2                     0                0       0      0     0           0   \n",
      "3                     0                0       0      0     0           0   \n",
      "4                     0                0       0      0     0           0   \n",
      "\n",
      "   cold  other_weather  direct_report  \n",
      "0     0              0              0  \n",
      "1     0              0              0  \n",
      "2     0              0              0  \n",
      "3     0              0              0  \n",
      "4     0              0              0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.head())\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize by converting to lower case\n",
    "# Tokenize by converting sentence to tokens\n",
    "# Remove stop words\n",
    "# Convert words to root form by Stemming\n",
    "def tokenize(text):\n",
    "    text=text.lower()\n",
    "    token=word_tokenize(text)\n",
    "    final_token=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for tok in token:\n",
    "        if tok not in stopwords.words('english'):\n",
    "            stem=stemmer.stem(tok)\n",
    "            final_token.append(stem)\n",
    "    return final_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf',MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x0000019AEAF835E8>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Measure performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting output on test set\n",
    "y_pred=pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,X_test, y_test):\n",
    "    y_pred=model.predict(X_test)\n",
    "    y_pred_df=pd.DataFrame(y_pred)\n",
    "    y_pred_df.columns=y_test.columns\n",
    "    \n",
    "    ## Creating an evaluation matrix of precision scores and recall scores for each column\n",
    "    eval_matrix=[]\n",
    "    for column in y_test.columns:    \n",
    "        eval_matrix.append(str(precision_score(y_test[column], y_pred_df[column])) +','+ str(recall_score(y_test[column], y_pred_df[column])) +','+ str(f1_score(y_test[column], y_pred_df[column])))\n",
    "    \n",
    "    # Converting eval matrix to data frame for ease of readability\n",
    "    df=pd.DataFrame(eval_matrix)\n",
    "    eval_df=df[0].str.split(',',expand=True)\n",
    "    eval_df.columns=['Precision','Recall','F1']\n",
    "    for col in eval_df.columns:\n",
    "        eval_df[col]=eval_df[col].astype(float)\n",
    "\n",
    "    print(eval_df.shape)\n",
    "    print(eval_df)\n",
    "    print(eval_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 3)\n",
      "    Precision    Recall        F1\n",
      "0    0.821100  0.966986  0.888092\n",
      "1    0.877895  0.461283  0.604786\n",
      "2    0.000000  0.000000  0.000000\n",
      "3    0.767830  0.688435  0.725968\n",
      "4    0.828571  0.068235  0.126087\n",
      "5    0.789474  0.062241  0.115385\n",
      "6    0.875000  0.089744  0.162791\n",
      "7    0.500000  0.010417  0.020408\n",
      "8    0.684211  0.075145  0.135417\n",
      "9    0.000000  0.000000  0.000000\n",
      "10   0.898438  0.333333  0.486258\n",
      "11   0.853598  0.587031  0.695652\n",
      "12   0.800000  0.278261  0.412903\n",
      "13   0.692308  0.116883  0.200000\n",
      "14   0.833333  0.043103  0.081967\n",
      "15   0.000000  0.000000  0.000000\n",
      "16   0.600000  0.016216  0.031579\n",
      "17   0.833333  0.116732  0.204778\n",
      "18   0.571429  0.022599  0.043478\n",
      "19   0.333333  0.005865  0.011527\n",
      "20   0.800000  0.066667  0.123077\n",
      "21   0.833333  0.091912  0.165563\n",
      "22   1.000000  0.019802  0.038835\n",
      "23   0.000000  0.000000  0.000000\n",
      "24   0.000000  0.000000  0.000000\n",
      "25   0.000000  0.000000  0.000000\n",
      "26   0.000000  0.000000  0.000000\n",
      "27   0.000000  0.000000  0.000000\n",
      "28   0.841402  0.692308  0.759608\n",
      "29   0.883333  0.488479  0.629080\n",
      "30   0.765079  0.494867  0.600998\n",
      "31   1.000000  0.022727  0.044444\n",
      "32   0.900943  0.773279  0.832244\n",
      "33   0.666667  0.039604  0.074766\n",
      "34   0.714286  0.017241  0.033670\n",
      "35   0.824324  0.371574  0.512246\n",
      "       Precision     Recall         F1\n",
      "count  36.000000  36.000000  36.000000\n",
      "mean    0.605256   0.195027   0.243378\n",
      "std     0.351954   0.270050   0.290215\n",
      "min     0.000000   0.000000   0.000000\n",
      "25%     0.458333   0.009279   0.018188\n",
      "50%     0.778652   0.064454   0.119231\n",
      "75%     0.835351   0.342893   0.492755\n",
      "max     1.000000   0.966986   0.888092\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(pipeline,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x0000019AEAF835E8>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', MultiOutputClassifier(estimator=RandomForestClassifier()))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(tokenizer=<function tokenize at 0x0000019AEAF835E8>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier()),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] vect__analyzer=word .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. vect__analyzer=word, score=0.261, total=12.6min\n",
      "[CV] vect__analyzer=word .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 12.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. vect__analyzer=word, score=0.253, total=12.7min\n",
      "[CV] vect__analyzer=word .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 25.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. vect__analyzer=word, score=0.260, total=12.8min\n",
      "[CV] vect__analyzer=word .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 38.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. vect__analyzer=word, score=0.248, total=12.7min\n",
      "[CV] vect__analyzer=word .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 50.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. vect__analyzer=word, score=0.246, total=12.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 63.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 63.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vect__analyzer': 'word'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'vect__analyzer': ['word']\n",
    "             ,'clf__estimator__min_samples_leaf': [1,3],\n",
    "             'clf__estimator__n_estimators':[10, 25], \n",
    "             'clf__estimator__min_samples_split':[2, 5]\n",
    "             }\n",
    "cv = GridSearchCV(pipeline,parameters,verbose=10)\n",
    "tuned_model=cv.fit(X_train,y_train)\n",
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Get the Precision, Recall and F1 score of the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 3)\n",
      "    Precision    Recall        F1\n",
      "0    0.822041  0.966230  0.888323\n",
      "1    0.864372  0.472345  0.610873\n",
      "2    0.000000  0.000000  0.000000\n",
      "3    0.773374  0.690249  0.729451\n",
      "4    0.710526  0.063529  0.116631\n",
      "5    0.789474  0.062241  0.115385\n",
      "6    0.826087  0.121795  0.212291\n",
      "7    0.500000  0.010417  0.020408\n",
      "8    0.636364  0.040462  0.076087\n",
      "9    0.000000  0.000000  0.000000\n",
      "10   0.921986  0.376812  0.534979\n",
      "11   0.862069  0.554608  0.674974\n",
      "12   0.828221  0.293478  0.433387\n",
      "13   0.545455  0.077922  0.136364\n",
      "14   0.800000  0.034483  0.066116\n",
      "15   0.000000  0.000000  0.000000\n",
      "16   0.583333  0.037838  0.071066\n",
      "17   0.916667  0.128405  0.225256\n",
      "18   0.656250  0.029661  0.056757\n",
      "19   0.166667  0.002933  0.005764\n",
      "20   0.818182  0.075000  0.137405\n",
      "21   0.814815  0.080882  0.147157\n",
      "22   0.666667  0.019802  0.038462\n",
      "23   0.000000  0.000000  0.000000\n",
      "24   0.000000  0.000000  0.000000\n",
      "25   0.000000  0.000000  0.000000\n",
      "26   0.000000  0.000000  0.000000\n",
      "27   0.250000  0.004032  0.007937\n",
      "28   0.843271  0.679945  0.752852\n",
      "29   0.899083  0.451613  0.601227\n",
      "30   0.766234  0.484600  0.593711\n",
      "31   0.000000  0.000000  0.000000\n",
      "32   0.902326  0.785425  0.839827\n",
      "33   0.875000  0.069307  0.128440\n",
      "34   0.857143  0.020690  0.040404\n",
      "35   0.823394  0.364467  0.505278\n",
      "       Precision     Recall         F1\n",
      "count  36.000000  36.000000  36.000000\n",
      "mean    0.575528   0.194421   0.243522\n",
      "std     0.352832   0.268716   0.290362\n",
      "min     0.000000   0.000000   0.000000\n",
      "25%     0.229167   0.003757   0.007393\n",
      "50%     0.769804   0.062885   0.116008\n",
      "75%     0.831983   0.367553   0.512703\n",
      "max     0.921986   0.966230   0.888323\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tuned_model,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle best model\n",
    "pickle.dump(tuned_model, open('models/disaster_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
